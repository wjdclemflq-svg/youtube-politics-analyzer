name: Dynamic YouTube Channel Discovery & Analysis

on:
  schedule:
    # ÌïúÍµ≠ ÏãúÍ∞Ñ Í∏∞Ï§Ä 6ÏãúÍ∞ÑÎßàÎã§ Ïã§Ìñâ (00:00, 06:00, 12:00, 18:00 KST)
    - cron: '0 15,21,3,9 * * *'  # UTC ÏãúÍ∞Ñ
  
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Enable debug mode'
        required: false
        default: 'false'
      channel_limit:
        description: 'Maximum channels to analyze'
        required: false
        default: '50'

permissions:
  contents: write
  actions: read

jobs:
  discover-and-analyze:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: üéØ Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: |
          npm init -y || true
          npm install googleapis@^105.0.0
          npm install dotenv
      
      - name: üóÇÔ∏è Create data directories
        run: |
          mkdir -p data/daily
          mkdir -p data/backup
      
      - name: üîç Run Dynamic Discovery & Analysis
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
          DEBUG_MODE: ${{ github.event.inputs.debug_mode || 'false' }}
          CHANNEL_LIMIT: ${{ github.event.inputs.channel_limit || '50' }}
        run: |
          echo "üöÄ Starting Dynamic Collection Process"
          echo "‚è∞ Time: $(date '+%Y-%m-%d %H:%M:%S')"
          echo "üìä Channel Limit: $CHANNEL_LIMIT"
          
          # Î©îÏù∏ ÏàòÏßë Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
          node scripts/collect-dynamic.js
          
          # Ïã§Ìñâ Í≤∞Í≥º ÌôïÏù∏
          if [ -f "data/latest.json" ]; then
            echo "‚úÖ Data collection successful"
            echo "üìä File size: $(du -h data/latest.json | cut -f1)"
            
            # Í∞ÑÎã®Ìïú ÌÜµÍ≥Ñ Ï∂úÎ†•
            node -e "
              const data = require('./data/latest.json');
              console.log('üìà Collection Stats:');
              console.log('  ‚Ä¢ Channels:', data.channels?.length || 0);
              console.log('  ‚Ä¢ Videos:', data.videos?.length || 0);
              console.log('  ‚Ä¢ Spikes:', data.spikes?.length || 0);
              console.log('  ‚Ä¢ Above Average:', data.aboveAverage?.length || 0);
            "
          else
            echo "‚ö†Ô∏è Warning: latest.json not found"
          fi
      
      - name: üìä Generate summary report
        run: |
          node -e "
            const fs = require('fs');
            const path = require('path');
            
            try {
              const data = JSON.parse(fs.readFileSync('data/latest.json', 'utf8'));
              
              const report = {
                timestamp: new Date().toISOString(),
                collection: {
                  channels: data.channels?.length || 0,
                  videos: data.videos?.length || 0,
                  spikes: data.spikes?.length || 0
                },
                topChannels: data.channels?.slice(0, 5).map(ch => ({
                  title: ch.title,
                  subscribers: ch.subscriberCount,
                  views: ch.viewCount
                })) || []
              };
              
              console.log('üìã Collection Report:');
              console.log(JSON.stringify(report, null, 2));
              
              // GitHub Actions ÏöîÏïΩÏóê Ï∂îÍ∞Ä
              if (process.env.GITHUB_STEP_SUMMARY) {
                const summary = \`
# üìä YouTube Collection Report
**Time: \${report.timestamp}
## üìà Statistics
- **Channels:** \${report.collection.channels}
- **Videos:** \${report.collection.videos}  
- **Trending Spikes:** \${report.collection.spikes}
## üèÜ Top Channels
\${report.topChannels.map((ch, i) => 
  \`\${i+1}. **\${ch.title}** - \${ch.subscribers?.toLocaleString() || 0} subscribers\`
).join('\\n')}
\`;
                fs.appendFileSync(process.env.GITHUB_STEP_SUMMARY, summary);
              }
              
            } catch (error) {
              console.error('Error generating report:', error.message);
            }
          "
      
      - name: üîÑ Backup old data
        run: |
          if [ -f "data/latest.json" ]; then
            BACKUP_FILE="data/backup/backup_$(date +%Y%m%d_%H%M%S).json"
            cp data/latest.json "$BACKUP_FILE"
            echo "üì¶ Backed up to: $BACKUP_FILE"
            
            # 7Ïùº Ïù¥ÏÉÅ Îêú Î∞±ÏóÖ ÏÇ≠Ï†ú
            find data/backup -name "backup_*.json" -mtime +7 -delete
            echo "üóëÔ∏è Cleaned up old backups"
          fi
      
      - name: üíæ Commit and push changes
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          
          # Î≥ÄÍ≤ΩÏÇ¨Ìï≠ ÌôïÏù∏
          git add data/
          
          if git diff --staged --quiet; then
            echo "üìù No changes to commit"
          else
            # Ïª§Î∞ã Î©îÏãúÏßÄÏóê ÌÜµÍ≥Ñ Ìè¨Ìï®
            CHANNELS=$(node -e "const d=require('./data/latest.json'); console.log(d.channels?.length || 0)")
            VIDEOS=$(node -e "const d=require('./data/latest.json'); console.log(d.videos?.length || 0)")
            
            git commit -m "üîÑ Update: $CHANNELS channels, $VIDEOS videos analyzed
            
Time: $(date '+%Y-%m-%d %H:%M:%S KST')
Channels: $CHANNELS
Videos: $VIDEOS
Action: ${{ github.run_number }}"
            
            git push origin main
            echo "‚úÖ Changes pushed successfully"
          fi
      
      - name: üè∑Ô∏è Create release (weekly)
        if: github.event.schedule == '0 15 * * 0'  # ÏùºÏöîÏùº ÏûêÏ†ï (KST)
        run: |
          TAG="v$(date +%Y.%W)"
          TITLE="Weekly Report - Week $(date +%W), $(date +%Y)"
          
          # Ï£ºÍ∞Ñ ÌÜµÍ≥Ñ ÏÉùÏÑ±
          node -e "
            const fs = require('fs');
            const data = JSON.parse(fs.readFileSync('data/latest.json', 'utf8'));
            
            const stats = {
              channels: data.channels?.length || 0,
              totalViews: data.channels?.reduce((sum, ch) => sum + (ch.viewCount || 0), 0) || 0,
              avgGrowth: (data.channels?.reduce((sum, ch) => sum + parseFloat(ch.dailyGrowthRate || 0), 0) / (data.channels?.length || 1)).toFixed(2)
            };
            
            const body = \`## üìä Weekly Statistics
- Total Channels Monitored: \${stats.channels}
- Total Views: \${stats.totalViews.toLocaleString()}
- Average Daily Growth Rate: \${stats.avgGrowth}%
            
### üèÜ Top Growing Channels
\${data.channels?.slice(0, 10).map((ch, i) => 
  \`\${i+1}. **\${ch.title}** - Growth: \${ch.dailyGrowthRate}%\`
).join('\\n') || 'No data'}\`;
            
            fs.writeFileSync('release_body.txt', body);
          "
          
          BODY=$(cat release_body.txt)
          
          gh release create "$TAG" \
            --title "$TITLE" \
            --notes "$BODY" \
            data/latest.json \
            data/summary.json
      
      - name: üìß Error notification
        if: failure()
        run: |
          echo "‚ùå Workflow failed!"
          echo "Error details:"
          echo "  - Job: ${{ github.job }}"
          echo "  - Run: ${{ github.run_number }}"
          echo "  - Time: $(date)"

  cleanup:
    runs-on: ubuntu-latest
    needs: discover-and-analyze
    if: always()
    
    steps:
      - name: üßπ Clean up workflow runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 7
          keep_minimum_runs: 10
